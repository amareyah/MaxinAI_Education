{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3ylxwxFjfD9BmhdbfGY++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amareyah/MaxinAI_Education/blob/master/dl_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2sPo3FxjYSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install fastai2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okXwo4AA2TQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_dim(in_shape, ker_shape, stride, padding):\n",
        "    \"\"\"\n",
        "    Calculate output dimension of the convolutional layer\n",
        "    Args:\n",
        "       in_shape: input shape - height or width\n",
        "       ker_shape: kernel shape - height or width\n",
        "       stride: stride\n",
        "       padding: padding\n",
        "    \n",
        "    Returns:\n",
        "        output dimension of convolutional layer\n",
        "    \"\"\"\n",
        "    out_shape = (in_shape - ker_shape + 2 * padding) / stride + 1\n",
        "    out_shape = int(out_shape)\n",
        "    \n",
        "    return out_shape\n",
        "\n",
        "out_dim = cnn_dim(224, 3, 1, 1)\n",
        "print(f'out_dim = {out_dim}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWl6WNdszZI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import vgg16\n",
        "model = vgg16(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYJuGHgyza-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import resnet50\n",
        "model = resnet50(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4kaqteb3XfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import resnext101_32x8d\n",
        "model = resnext101_32x8d(pretrained=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnBw65Zg3u-F",
        "colab_type": "text"
      },
      "source": [
        "There are different ResNet models:\n",
        "- ResNet18\n",
        "- ResNet34\n",
        "- ResNet50\n",
        "- ResNet101\n",
        "- ResNet1001\n",
        "\n",
        "Inception-ResNet\n",
        "- Inception-ResNet A Block\n",
        "- Inception-ResNet B Block\n",
        "- Inception-ResNet C Block\n",
        "- Reduction A Block\n",
        "- Reduction B Block\n",
        "\n",
        "ResNeXt architectures:\n",
        "- ResNext50_32X4d\n",
        "- ResNext50_64X4d\n",
        "- ResNext101_32X4d\n",
        "- ResNext101_64X4d\n",
        "\n",
        "Other architectures\n",
        "- DenceNet\n",
        "- TraceNet\n",
        "- EfficientNet\n",
        "\n",
        "see details in **lecture_16_convolutional_neural_networks.ipynb**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxQXKVe5-2ir",
        "colab_type": "text"
      },
      "source": [
        "**Global Average Pooling Layer:**  For each feature map takes the average value of all nodes and maps that value to one node. Number of in_channels = Number of out_channels.\n",
        "\n",
        "**Adaptive Average Pooling Layer:**\n",
        "For each feature map applies a 2D adaptive average pooling over an input signal. The output is of size H x W, **for any input size**. The number of output features is equal to the number of input planes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4UswnklFvHs",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction / embedding\n",
        "Let's take one of the pre-trained (on ImageNet) models, VGG, Inception, ResNet, etc and remove all the last layers before convolutional layers:\n",
        "\n",
        "- For VGG16 remove last two fully connected layers\n",
        "- For Inception and ResNet remove all the layer after adaptive (global) average pooling\n",
        "\n",
        "So our model generates vector from the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld4hilTSGCcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models import resnet50, resnet34, vgg16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHZWY47TGHXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = vgg16(pretrained=True)\n",
        "net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vPuqWVSGO4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(*list(net.children())[:-1])\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjvJxIklGSXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn(1, 3, 399, 399)\n",
        "with torch.no_grad():\n",
        "    y1 = net(x)\n",
        "    y2 = model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miF4KaACGVQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y1.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKxTmFXZGWbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjEpto3dGYSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2 = torch.flatten(y2, 1)\n",
        "y2.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG2YOt_HHCd3",
        "colab_type": "text"
      },
      "source": [
        "So we have $2048$ dimensional vectors, we can run our model on the our dataset of images and generate $2048$ dimensional vectors.\n",
        "$$\n",
        "f: \\mathbb{R}^{3 \\times H \\times W} \\mapsto \\mathbb{R}^d\n",
        "$$\n",
        "<br>\n",
        "Our model maps each $C \\times H \\times W$ (they might be different for adaptive average pooling) dimensional image to the fixed $d$ dimensional vector. \n",
        "\n",
        "Vectors have \"distance\" property.\n",
        "<br>\n",
        "If we store this vectors and run **K-nearest neighbor** search we can observe that similarity search is working even if our dataset was not used during the training.\n",
        "<br>\n",
        "Note: Search results depend on model and domain of training set and dataset\n",
        "<br>\n",
        "\n",
        "Last layer **Dimensionality Reduction**\n",
        "\n",
        "**Visualize the space of feature vectors** by reducing dimensionality of vectors from 4096 to 2 dimensions.\n",
        "Simpple algorith: **Principle Component Analysis (PCA)**\n",
        "More comlex: **t-SNE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEh3w3o9JFl2",
        "colab_type": "text"
      },
      "source": [
        "## Transfer-learning\n",
        "\n",
        "We can see that first layers extract essential features which are pretty similar for all images. Second layers extract more complex features and last layers more domain-specific features\n",
        "Can we use this information for different task. Would it be enough information, enough features if use it pre-trained model on the different dataset?\n",
        "\n",
        "With the following approach:\n",
        "- We extract features from the images with the pre-trained model\n",
        "- Train different model with this features\n",
        "\n",
        "Turns out that this approach works and it's called transfer earning. For transfer learning we should consider the following:\n",
        "- Is the model is trained on the similar domain\n",
        "- Is the model trained on the enough data\n",
        "\n",
        "The state-of-the art result achieved with model trained on ImageNet classification task:\n",
        "- It has different and well-distributed images\n",
        "- More precise labeled\n",
        "- Or it has enough images to extract \"all-possible\" features\n",
        "\n",
        "There are several approaches:\n",
        "- Use extracted features and train different model\n",
        "- Freeze the weights and train only classifier\n",
        "- Fine-tune whole model with discriminative learning rates\n",
        "\n",
        "First approach needs pre-extraction of the feature vectors and training different model on them:\n",
        "\n",
        "- Extract features\n",
        "- Train different classifier (SVM, RF, GB) on them\n",
        "\n",
        "For the second approach we put our layers on top the model and train it:\n",
        "- Put custom layers on model\n",
        "- Freeze feature extraction layers weights\n",
        "- Train custom layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaeB60SGLdMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fn = nn.Sequential(*list(model.children()) + [nn.Linear(25088, 500), nn.Dropout(p=0.3), nn.Linear(500, 20)])\n",
        "model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101zXa0WL3Dw",
        "colab_type": "text"
      },
      "source": [
        "For third approach, we put our layers on top the model and train it with different learning rate:\n",
        "\n",
        "- Put custom layers on model\n",
        "- Train full model using larger learning rate for last layers, smaller maybe  $\\frac{1}{100}$  for the middle layers and  $\\frac{1}{1000}$  for the first layers.\n",
        "\n",
        "Pre-trained classifier also used for different tasks\n",
        "- Segmentation\n",
        "- Detection\n",
        "- Image search / metric learning\n",
        "- Auto-encoders\n",
        "- GAN\n",
        "- etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXgxDcXCM9KU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_openml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0kVSl0pM-z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y_str = fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhR1jkSvNBB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img1 = X[1].reshape(28, 28)\n",
        "plt.imshow(img1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}